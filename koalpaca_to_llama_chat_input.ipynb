{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/id4thomas/miniforge3/envs/torch2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your huggingface api key here\n",
    "AUTH_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/id4thomas/.cache/huggingface/datasets/beomi___parquet/beomi--KoAlpaca-v1.1a-1465f66eb846fd61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 148.06it/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"beomi/KoAlpaca-v1.1a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "# We use the default prompt in llama git repo\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Koalpaca preprocessing\n",
    "def koalpaca_map_fn(x):\n",
    "    text_format = lambda x: f\"### 질문: {x['instruction']}\\n\\n### 답변: {x['output']}<|endoftext|>\"\n",
    "    return {'text': text_format(x) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_dialog_preprocess(dialog):\n",
    "\tif dialog[0][\"role\"] != \"system\":\n",
    "\t\tdialog = [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"role\": \"system\",\n",
    "\t\t\t\t\"content\": DEFAULT_SYSTEM_PROMPT,\n",
    "\t\t\t}\n",
    "\t\t] + dialog\n",
    "\tdialog = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": dialog[1][\"role\"],\n",
    "\t\t\t\"content\": B_SYS\n",
    "\t\t\t+ dialog[0][\"content\"]\n",
    "\t\t\t+ E_SYS\n",
    "\t\t\t+ dialog[1][\"content\"],\n",
    "\t\t}\n",
    "\t] + dialog[2:]\n",
    "\tassert all([msg[\"role\"] == \"user\" for msg in dialog[::2]]) and all(\n",
    "\t\t[msg[\"role\"] == \"assistant\" for msg in dialog[1::2]]\n",
    "\t), (\n",
    "\t\t\"model only supports 'system', 'user' and 'assistant' roles, \"\n",
    "\t\t\"starting with 'system', then 'user' and alternating (u/a/u/a/u...)\"\n",
    "\t)\n",
    "\treturn dialog\n",
    "\n",
    "def map_dialog_to_tokenizer_input(dialog):\n",
    "\treturn f\"{B_INST} {(dialog[0]['content']).strip()} {E_INST} {(dialog[1]['content']).strip()}\"\n",
    "\n",
    "def map_data_to_lambda_text(x):\n",
    "\tinstruction = x[\"instruction\"]\n",
    "\toutput = x[\"output\"]\n",
    "\n",
    "\tdialog = [\n",
    "\t\t# {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "\t\t{\"role\": \"user\", \"content\": instruction},\n",
    "\t\t{\"role\": \"assistant\", \"content\": output}\n",
    "\t]\n",
    "\tprocessed_dialog = llama_dialog_preprocess(dialog)\n",
    "\tdialog_input = map_dialog_to_tokenizer_input(processed_dialog)\n",
    "\treturn dialog_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요? [/INST] 양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \n",
      "\n",
      "식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\n",
      "\n",
      " 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \n",
      "\n",
      "고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.\n"
     ]
    }
   ],
   "source": [
    "sample = data[\"train\"][0]\n",
    "instruction = sample[\"instruction\"]\n",
    "output = sample[\"output\"]\n",
    "\n",
    "dialog = [\n",
    "\t# {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT}, # Use system if needed\n",
    "\t{\"role\": \"user\", \"content\": instruction},\n",
    "\t{\"role\": \"assistant\", \"content\": output}\n",
    "]\n",
    "processed_dialog = llama_dialog_preprocess(dialog)\n",
    "# print(processed_dialog)\n",
    "\n",
    "processed_dialog_input = map_dialog_to_tokenizer_input(processed_dialog)\n",
    "print(processed_dialog_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/id4thomas/miniforge3/envs/torch2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\t\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "\tuse_auth_token = AUTH_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 1\n",
      "</s> 2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/id4thomas/.cache/huggingface/datasets/beomi___parquet/beomi--KoAlpaca-v1.1a-1465f66eb846fd61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-316fcef9c9555a24.arrow\n"
     ]
    }
   ],
   "source": [
    "data = data.map(\n",
    "    lambda x: {'text': tokenizer.bos_token+map_data_to_lambda_text(x)+tokenizer.eos_token}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요? [/INST] 양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.</s>\",\n",
       " \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n스웨터의 유래는 어디에서 시작되었나요? [/INST] 스웨터의 유래는 14세기경 북유럽항구지역에서 어망을 짜던 기술을 의복에 활용하면서 시작되었습니다. 노동자들의 방한복에서 시작된 스웨터는 여가생활과 스포츠의 붐에 힘입어 대중화되었습니다. 이후, 겨울철 이너웨어의 대명사가 되었습니다. 스웨터는 짜서(Knit) 만든 옷을 말하며, 어부들의 방한복으로 짜여졌던 스웨터 중에서도 스코틀랜드 해안지방의 여인들은 바다로 나가는 남편이나 연인, 자식들에게 무사히 돌아올 것을 기원하며 로프나 닻 무늬를 정성껏 짜넣었다고 합니다. 그 실용성과 정성이 오늘에까지 이어지고 있습니다.</s>\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][:2][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/id4thomas/.cache/huggingface/datasets/beomi___parquet/beomi--KoAlpaca-v1.1a-1465f66eb846fd61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-1c10eb386fe5a352.arrow\n"
     ]
    }
   ],
   "source": [
    "train_ds = data[\"train\"].map(lambda samples: tokenizer(samples[\"text\"], add_special_tokens=False), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 29892, 3390, 1319, 322, 15993, 20255, 29889, 29849, 1234, 408, 1371, 3730, 408, 1950, 29892, 1550, 1641, 9109, 29889, 3575, 6089, 881, 451, 3160, 738, 10311, 1319, 29892, 443, 621, 936, 29892, 11021, 391, 29892, 7916, 391, 29892, 304, 27375, 29892, 18215, 29892, 470, 27302, 2793, 29889, 3529, 9801, 393, 596, 20890, 526, 5374, 635, 443, 5365, 1463, 322, 6374, 297, 5469, 29889, 13, 13, 3644, 263, 1139, 947, 451, 1207, 738, 4060, 29892, 470, 338, 451, 2114, 1474, 16165, 261, 296, 29892, 5649, 2020, 2012, 310, 22862, 1554, 451, 1959, 29889, 960, 366, 1016, 29915, 29873, 1073, 278, 1234, 304, 263, 1139, 29892, 3113, 1016, 29915, 29873, 6232, 2089, 2472, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 239, 153, 148, 240, 143, 143, 31081, 29871, 31129, 238, 153, 167, 29871, 31895, 238, 175, 191, 29871, 31279, 31724, 30918, 30903, 31527, 29973, 29871, 31607, 30826, 31137, 29871, 31137, 31231, 31417, 31081, 29871, 238, 194, 143, 30826, 30918, 30903, 31527, 29973, 518, 29914, 25580, 29962, 29871, 239, 153, 148, 240, 143, 143, 31081, 29871, 239, 161, 145, 30393, 29871, 30860, 238, 142, 143, 29871, 31895, 238, 175, 191, 30708, 29871, 239, 167, 135, 30827, 29871, 31279, 238, 185, 135, 239, 161, 136, 31063, 30709, 29889, 29871, 31137, 31231, 31417, 31081, 29871, 31895, 238, 175, 191, 30708, 29871, 238, 194, 143, 30826, 29871, 31279, 238, 185, 135, 239, 161, 136, 31063, 30709, 29889, 29871, 13, 13, 31895, 238, 175, 191, 30708, 29871, 31279, 31724, 30708, 29871, 31231, 238, 185, 135, 31054, 29871, 30890, 31435, 29871, 237, 185, 132, 237, 187, 139, 31435, 30944, 31081, 29871, 238, 185, 135, 30393, 31197, 31747, 29871, 238, 185, 135, 31976, 29871, 30393, 29871, 239, 170, 139, 31406, 31054, 29871, 30890, 30877, 29871, 238, 142, 184, 31286, 29871, 239, 179, 193, 31137, 29871, 239, 161, 139, 31286, 29871, 237, 181, 134, 239, 161, 136, 31063, 30709, 29889, 29871, 239, 153, 148, 240, 143, 143, 31081, 29871, 239, 161, 145, 30393, 29871, 30860, 238, 142, 143, 29871, 239, 167, 135, 30827, 29871, 31279, 238, 185, 135, 239, 161, 136, 31063, 30709, 29889, 29871, 31137, 31231, 31417, 31081, 29871, 30709, 238, 168, 187, 29871, 239, 170, 139, 31406, 31906, 29871, 238, 142, 184, 238, 182, 131, 31054, 31093, 29871, 239, 153, 187, 237, 187, 140, 238, 147, 159, 29871, 237, 181, 134, 31906, 29871, 237, 179, 156, 30393, 29871, 238, 194, 143, 30826, 29871, 31279, 238, 185, 135, 239, 161, 136, 31063, 30709, 29889, 29871, 238, 151, 179, 31197, 31093, 29892, 29871, 239, 153, 148, 240, 143, 143, 31081, 29871, 31895, 238, 175, 191, 30708, 29871, 239, 167, 135, 30827, 29871, 31279, 238, 185, 135, 30393, 29871, 238, 147, 155, 31137, 29892, 29871, 31137, 31231, 31417, 31081, 29871, 31895, 238, 175, 191, 30708, 29871, 238, 194, 143, 30826, 29871, 31279, 238, 185, 135, 239, 161, 136, 31063, 30709, 29889, 13, 13, 29871, 238, 144, 170, 238, 185, 156, 30393, 31081, 29871, 238, 142, 184, 238, 182, 131, 29901, 29871, 31137, 31231, 31417, 29871, 239, 167, 135, 30827, 31136, 29871, 238, 182, 185, 30860, 238, 171, 188, 31286, 29871, 30970, 29871, 239, 161, 139, 31207, 31527, 29973, 29871, 13, 13, 31137, 31231, 31417, 29871, 239, 167, 135, 30827, 31136, 29871, 31895, 31737, 239, 159, 191, 30906, 29871, 238, 182, 185, 30860, 238, 171, 188, 31286, 29871, 30970, 29871, 239, 161, 139, 239, 141, 184, 31063, 30709, 29889, 29871, 30944, 30811, 31826, 29871, 239, 167, 135, 30827, 29871, 238, 194, 147, 31826, 29871, 30860, 31063, 31197, 29892, 29871, 239, 161, 145, 29892, 29871, 31781, 29892, 29871, 238, 194, 143, 30826, 237, 188, 143, 30811, 29871, 31962, 238, 150, 163, 29871, 31279, 31724, 30903, 29871, 31895, 31737, 239, 159, 191, 30906, 29871, 240, 156, 159, 31737, 238, 147, 155, 30827, 31136, 29871, 31980, 31063, 30709, 29889, 29871, 30709, 31826, 29892, 29871, 30877, 31293, 31054, 31093, 31081, 29871, 31153, 238, 179, 155, 239, 163, 132, 239, 159, 191, 30906, 29871, 238, 194, 143, 30826, 29871, 31279, 238, 185, 135, 30918, 29871, 31137, 31231, 31417, 31517, 29871, 30981, 30906, 29871, 238, 171, 188, 239, 141, 184, 31063, 30709, 29889, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0][\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
